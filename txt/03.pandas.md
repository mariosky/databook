![Pandas](../img/pandas.jpg)
# Pandas

El análisis de datos comúnmente se realiza sobre datos que se encuentran organizados en forma de tablas. Probablemente ya haz utilizado datos organizados de esta manera al utilizar una hoja de cálculo o tablas en servidores de bases de datos relacionales como Oracle, PostgreSQL o SQLServer. En python incluso podemos almacenar los datos de una tabla utilizando un arreglo de tuplas.

## Otras opciones

### ¿Por que no utilizamos tablas de SQL?
Los sistemas de bases de datos tienen como objetivo almacenar grandes cantidades
de datos y al mismo tiempo nos permiten realizar un gran número de transacciones
de manera concurrente. Dicho de otra manera los servidores nos permiten realizar
la parte operativa de una empresa, vender boletos para conciertos, inscribir
alumnos, llevar el control de un almacén, procesar los pedidos en una tienda en
línea, etc. Además de esto cuentan con un lenguaje de alto nivel para definir,
manipular y consultar a los datos. El lenguaje estándar SQL ha sido el preferido
por los desarrolladores para consultar de una manera muy flexible a los datos
relacionales. El problema que tenemos en estos sistemas es que su objetivo
principal no es el analizar los datos, estos sistemas nos permiten más bien
implementar sistemas que necesitan manipular datos en-línea para funcionar. Por
otro lado, para hacer nuestro análisis, podemos utilizar perfectamente datos que
estén fuera de línea, datos históricos o incluso datos ficticios. Mucho del
*overhead* de un servidor de bases de datos se tiene precisamente por que estos
deben ser capaces de modificar las estructuras de las tablas, hacer
modificaciones a objetos, mantener la consistencia, controlar el acceso y muchas
otras operaciones. Todo en línea. Al no tener estos requisitos podemos operar
con estructuras diseñadas precisamente para el análisis de datos.  Dicho esto,
algunos sistemas de bases de datos han agregado la funcionalidad necesaria para
realizar el tipo de análisis que veremos, pero esto no es todavía el estándar.
Los sistemas de bases de datos nos pueden servir para almacenar los datos que
utilizaremos en el análisis y realizar algunas consultas con SQL. Podemos
utilizar también la biblioteca SQLite para almacenar bitácoras, intercambiar
información con otros sistemas o como parte del preprocesamiento.

### ¿Por que no utilizamos NoSQL?
Últimamente se han propuesto gestores de datos que aunque no siguen el modelo
relacional nos permiten hacer cierto tipo de consultas de una manera mucho más
eficiente. Por ejemplo, los almacenes clave-valor (key-value) pueden recuperar
bastante rápido los datos por medio de su clave; pero el lenguaje de consulta,
si es que lo hay, no es muy flexible. Son muy buenos para recuperar muy rápido
un documento html o un objeto con solo especificar su clave, pero no podemos
hacer consultas *ad-hoc* como "recupera aquellos productos que tengan ventas
superiores al promedio en las tiendas de California o Arizona, siempre que no
sean del departamento de electrónica". Estos sistemas nos pueden servir sobre
todo como memoria cache o memoria compartida para realizar operaciones en
paralelo.

### ¿Por que no utilizamos Data Warehouse?
Las técnicas de Almacenes de Datos (Data Warehouse) nos permiten realizar de una
manera muy sencilla consultas *tipo cubo* sobre bases de datos relacionales.
Aunque las consultas permiten a los ejecutivos tomar decisiones y tener una
buena idea de lo que sucede en la empresa, su objetivo no es el de extraer
patrones de los datos. Sin embargo el objetivo general y el diseño de los
Almacenes de Datos tiene algunas cosas en común con el proceso de KDD. Ambos
trabajan con datos históricos y requieren de un proceso previo de extracción de
los datos al cual se le conoce como ETL (Extract, Transform and Load) o
Extraer, Transformar y Cargar). El proceso de ETL permite a las empresas extraer
datos desde diferentes fuentes, cambiar de formato  y limpiarlos con el objetivo
de  cargarlos en el almacén de datos.

### ¿Por que no utilizamos Hoja de Cálculo?
De hecho las hojas de cálculo son una herramienta aceptable para realizar
análisis de datos. Muchos analistas utilizan hojas de calculo como herramienta
principal. No se requiere tener un amplío conocimiento de programación para
sacarles provecho y pueden complementares con otras herramientas por ejemplo con
sistemas de bases de datos. Los programadores normalmente preferimos tener un
control total sobre el proceso de KDD y nos gusta poder integrar fácilmente
nuestro código en distintas aplicaciones, por lo que preferimos la flexibilidad
que nos brinda un lenguaje de propósito general.

# Breve Introducción al DataFrame de Pandas

Pandas es una biblioteca código abierto en Python la cual se ha diseñado con el objetivo específico de brindarnos una estructura de tabla rápida y flexible para el análisis de datos. Según
su [documentación](http://pandas.pydata.org/pandas-docs/stable/)
pandas nos permite operar con distintos tipos de datos:

* Tablas con atributos heterogéneos.
* Series de tiempo, ordenadas o no.
* Matrices homogéneas o heterogéneas.

Para esto en pandas se implementan dos estructuras principales: *Series* y *DataFrame*. La estructura *Series* nos permite trabajar con datos de series de tiempo especificadas como un vector. Por otro lado está *DataFrame* nos sirve para aquellos datos que tienen una estructura de tabla o matriz. En esta sección nos enfocaremos en la estructura *DataFrame* ya que nos interesa trabajar con datos organizados como tabla.

## Creando un DataFrame desde un archivo de texto
Como ejemplo, vamos a utilizar una estructura tipo DataFrame para almacenar el conjunto de
datos conocido como [Auto
MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG). Lo primero que debemos  hacer es ver los tipos de datos de los atributos:

|  nombre        | tipo     | medida     |  descripción             |
|----------------|----------| -----------|--------------------------|
|  mpg           | continuo | razón      | millas por galón         |
|  cylinders     | discreto | ordinal    | número de cilindros      |
|  displacement  | continuo | razón      | desplazamiento           |
|  horsepower    | continuo | razón      | caballos de fuerza       |
|  weight        | continuo | razón      | peso en libras US        |
|  acceleration  | continuo | razón      | aceleración              |
|  model_year    | discreto | razón      | año de fabricación       |
|  origin        | discreto | categórico | origen                   |
|  car_name      | cadena   | categórico | nombre único del auto    |

Como vemos estas observaciones tienen atributos heterogéneos ya que son de distintos tipos. El objetivo original de este conjunto de datos era el de predecir el consumo de combustible en millas por galón (mpg) utilizando los otros atributos.

Como siguiente paso vamos a descargar el conjunto de datos del [repositorio de
machine learning de la
UC-Irvine](https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/).
El archivo se llama **auto-mpg.data**. Al abrir el archivo en un editor de texto
vemos que está separado por espacios y faltan algunos valores. Veamos un
fragmento:

```
11.0   8   350.0      180.0      3664.      11.0   73  1	"oldsmobile omega"
20.0   6   198.0      95.00      3102.      16.5   74  1	"plymouth duster"
21.0   6   200.0      ?          2875.      17.0   74  1	"ford maverick"
```

Pandas nos brinda *IO Tools* un API de entrada/salida donde se incluyen métodos
de lectura para formatos de texto, binarios y SQL. En el caso de text se
incluyen lectores para formatos CSV, JSON y HTML. Vamos a intentar leer el
archivo utilizando el método *read_csv()* para más detalle puedes ver la
[documentación](http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table).
La manera en la que se lee y se hace el "parsing" a los archivos se puede configurar de una manera muy detallada, para este ejemplo solo modificaremos algunos parámetros básicos. Recordemos que en los archivos CSV, como el nombre lo dice, los valores de los atributos se separan por comas. Ya vimos que en el  caso del archivo **auto-mpg.data**  la separación se hace por medio de espacios en blanco. Un problema adicional es que el número de espacios no siempre es el mismo. Para leer el archivo correctamente, vamos a utilizar el parámetro *sep* el cual recibe una especificación del separador que se va a utilizar, por defecto una coma ','. En nuestro caso el separador será una expresión regular la cual le diga al método que son uno o más espacios. El espacio y tabuladores se especifican en la [expresión regular](https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular) con la cadena **'\s'**, mientras que el operador **'+'** indica que se puede repetir una o más veces. Nuestro primer intento quedaría de la siguiente manera:



``` python
In [1]: import matplotlib.pyplot as plt

In [2]: import pandas as pd

In [3]: import numpy as np

In [4]: df = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s')

```
Antes que nada importamos las bibliotecas que utilizaremos en estos
ejercicios (1-3). Como primer parámetro enviamos la ruta al archivo *auto-mpg.data*, si ejecutamos el comando dentro del directorio del libro  solamente especificamos la ruta de la siguiente manera: *'datos-ejemplo/auto-mpg.data'*. En caso de que se encuentre en otro lado simplemente cambiamos la ruta.

#### Nota:
En caso de que te marque algún error, tal vez debas actualizar las versiones de las bibliotecas de anaconda. Desde la línea de comando de tu sistema operativo ejecuta este comando:
```
conda update --all
```


``` python
In [5]: df
Out[5]:
     18.0  8  307.0  130.0   3504.  12.0  70  1  \
0    15.0  8  350.0  165.0  3693.0  11.5  70  1   
1    18.0  8  318.0  150.0  3436.0  11.0  70  1   
2    16.0  8  304.0  150.0  3433.0  12.0  70  
..
chevrolet chevelle malibu  
0                    buick skylark 320  
1                   plymouth satellite  
2                        amc rebel sst  
3                          ford torino  


[397 rows x 9 columns]
```

El método lector espera que el primer renglón tenga el nombre de los atributos, pero nuestro archivo no los tiene. Por lo que el nombre de los atributos no tienen mucho sentido *"18.0  8  307.0  130.0   3504.  12.0  70  1 chevrolet chevelle malibu"*. Debemos indicar que nuestro archivo no incluye el renglón de encabezados con el argumento *header=None*. En este caso, como no hemos especificado nombres aun los atributos tendrán como nombre simplemente un indice.

Vamos a corregir el problema indicando que no se tiene una línea de encabezado:

``` python
In [6]: df = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s+', header=None)

In [7]: df
Out[7]:
        0  1      2      3       4     5   6  7  \
0    18.0  8  307.0  130.0  3504.0  12.0  70  1   
1    15.0  8  350.0  165.0  3693.0  11.5  70  1   
..
8  
0            chevrolet chevelle malibu  
1                    buick skylark 320
..

[398 rows x 9 columns]
```

Vamos especificando los nombres de los atributos, esto se hace agregando el parámetro **names** y enviando una lista con los nombres:

``` python
In [8]: df2 = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s+', header=None, names=['mpg','cylinders','displacement','horsepower','weight','acceleration','model_year','origin','car_name'])

In [9]: df2
Out[9]:
      mpg  cylinders  displacement horsepower  weight  acceleration  \
0    18.0          8         307.0      130.0  3504.0          12.0   
1    15.0          8         350.0      165.0  3693.0          11.5   
2    18.0          8         318.0      150.0  3436.0          11.0   
```
Mucho mejor, veamos que tipo de datos infirió el método de lectura para cada una de las columnas:

``` python
In [10]: df2.dtypes
Out[10]:
mpg             float64
cylinders         int64
displacement    float64
horsepower       object
weight          float64
acceleration    float64
model_year        int64
origin            int64
car_name         object
dtype: object

```
Hay un problema con el atributo *horsepower* este debería de ser float64. Lo que
sucede es que al leer el carácter *?* el método de lectura no sabe que tipo de
dato inferir. Lo bueno es que podemos configurar como queremos que se resuelvan
los casos donde tenemos datos "no disponibles" y también como se debe pasar este tipo de valores. El argumento que vamos a utilizar es *na_values*, podemos indicar una
lista de valores o como en este caso simplemente el símbolo correspondiente.

``` python
In [11]: df2 = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s+', header=None, names=['mpg','cylinders','displacement','horsepower','weight','acceleration','model_year','origin','car_name'], na_values='?')

In [12]: df2.dtypes
Out[12]:
mpg             float64
cylinders         int64
displacement    float64
horsepower      float64
weight          float64
acceleration    float64
model_year        int64
origin            int64
car_name         object
dtype: object
```
Ahora si, cada una de las columnas corresponde al tipo de dato de los atributos de la tabla anterior. Si queremos también podemos indicar al momento de la lectura el nombre y tipo de dato de los atributos. Incluso podemos indicar si tenemos algún dato categórico.

``` python
In [13]: df2 = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s+', header=None,  na_values='?', names=['mpg','cylinders','displacement','horsepower','weight','acceleration','model_year','origin','car_name'], dtype={'mpg':'f4', 'cylinders':'i4','displacement':'f4','horsepower':'f4','weight':'f4','acceleration':'f4','model_year':'i4','origin':'category','car_name':'category'})

In [14]: df2.dtypes
Out[14]:
mpg              float32
cylinders          int32
displacement     float32
horsepower       float32
weight           float32
acceleration     float32
model_year         int32
origin          category
car_name        category
dtype: object

```
Una vez leído correctamente el *DataFrame* podemos utilizar el método **describe()** para generar un resumen estadístico descriptivo que incluye la tendencia central, dispersión y la distribución del conjunto de datos, excluyendo datos categóricos y valores no disponibles.

```
In [15]: df2.describe()
Out[15]:
              mpg   cylinders  displacement  horsepower       weight  \
count  398.000000  398.000000    398.000000  392.000000   398.000000   
mean    23.514574    5.454774    193.425873  104.469391  2970.424561   
std      7.815985    1.701004    104.269859   38.491138   846.841431   
min      9.000000    3.000000     68.000000   46.000000  1613.000000   
25%     17.500000    4.000000    104.250000   75.000000  2223.750000   
50%     23.000000    4.000000    148.500000   93.500000  2803.500000   
75%     29.000000    8.000000    262.000000  126.000000  3608.000000   
max     46.599998    8.000000    455.000000  230.000000  5140.000000   

       acceleration  model_year  
count    398.000000  398.000000  
mean      15.568086   76.010050  
std        2.757689    3.697627  
min        8.000000   70.000000  
25%       13.825000   73.000000  
50%       15.500000   76.000000  
75%       17.175001   79.000000  
max       24.799999   82.000000  
```

Ya que hablamos de los datos categóricos, vamos especificando el nombre de cada categoría. En el archivo solo se especifican los valores 1, 2 y 3. Al ver los modelos de los autos, inferimos que deben ser "USA", "Japan" y "Germany".

``` python
In [16]: df2["origin"].cat.categories = ["USA", "Japan", "Germany"]

In [17]: df2['origin']
Out[17]:
0          USA
1          USA
2          USA
3          USA
4          USA
..
390    Germany
391        USA
392        USA
393        USA
394      Japan
395        USA
396        USA
397        USA

Name: origin, Length: 398, dtype: category
Categories (3, object): [USA, Japan, Germany]
```
¡Muy bien!, ¿ahora que tal si hacemos una gráfica de pastel?. Para esto generamos la gráfica con el método **plot()**. Todo esto lo veremos a detalle en la sección de visualización de datos.    

``` python
In [18]: df2['origin'].value_counts().plot(kind='bar')
Out[18]: <matplotlib.axes._subplots.AxesSubplot at 0x10d1d1cd0>
```
Una vez creada la gráfica la mostramos con el método **show()** de la biblioteca *matplotlib*.

``` python
In [36]: plt.show()
```
Deberíamos ver la siguiente gráfica:

![pie](../img/pie.png)

### Nota:
Debes de cerrar la ventana donde se muestra la gráfica para desbloquear el interprete y poder continuar. Puedes grabar la gráfica si gustas.

Vamos ahora a graficar tres variables para ver si vemos algo interesante:

``` python
In [30]: df2.plot.scatter(x='weight', y='mpg',c='horsepower',cmap='viridis');
In [31]: plt.show()
```
![Plot](../img/cmap.png)

Intenta cambiar el mapa de colores  con otras opciones [Color Maps](http://matplotlib.org/examples/color/colormaps_reference.html).

Si quieres intentar algunas otras gráficas adelante:
http://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization

Como hemos visto una vez que tenemos nuestros datos en la estructura *DataFrame* podemos hacer cosas interesantes muy fácilmente.

En esta sección vimos un ejemplo sencillo de como leer los datos de un archivo ya que es una tarea muy común. Ahora veremos como almacenar los datos utilizando objetos de python ya que esto nos brindará una flexibilidad adicional, por ejemplo para leer los datos desde una base de datos No-SQL o por http.

## Creando un DataFrame con el constructor
El constructor de la clase  [DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) puede tomar los siguientes parámetros: **data**, **index**, **columns**, **dtype** y **copy**. Vemos cada uno de ellos:

### data
En este parámetro enviamos los datos, como sucede muchas veces en Python, estos pueden ser de varios tipos:

#### lista de tuplas
La manera más sencilla de enviar datos a un *DataFrame* es utilizando una lista de tuplas:

``` python
>>> import matplotlib.pyplot as plt
>>> import pandas as pd
>>> import numpy as np
>>> datos = [('ana','ann@example.com','active'),
             ('tom','tommy@example.com','active'),
             ('joe','jj@example.com','active')]
>>> df2 = pd.DataFrame(datos)
>>> df2
     0                  1       2
0  ana    ann@example.com  active
1  tom  tommy@example.com  active
2  joe     jj@example.com  active
>>>
```
La lista de tuplas puede venir de una consulta de base de datos u otro proceso.

#### numpy.ndarray
Los datos pueden venir también como un arreglo *n-dimensional* de la biblioteca [numpy](https://docs.scipy.org/doc/numpy/user/index.html). De hecho la estructura *DataFrame* utiliza internamente este tipo de objetos. Un *ndarray* es un contenedor multidimensional de objetos del mismo tipo y tamaño. Los arreglos multidimensionales también cuentan con un objeto de definición de tipo (data-type dtype).

Como ejemplo vamos a crear un arreglo *dimensión-2*, de tamaño 2x4 el cual contiene enteros de 32 bits:

``` python
>>> arreglo = np.array([[1, 2, 3, 4], [9, 9, 4, 8]], dtype=np.int32)
>>> type(arreglo)
<type 'numpy.ndarray'>
>>> arreglo.shape
(2, 4)
>>> arreglo.dtype
dtype('int32')
```  
Podemos revisar el tamaño y tipo de los objetos *numpy.ndarray* con los atributos *shape* y *dtype* respectivamente. Si te fijas la tupla que regresa el atributo *shape* nos dice el número de renglones primero y después el número de columnas.  

Ahora vamos a crear un *DataFrame* a partir de nuestro arreglo:
``` python
>>> df = pd.DataFrame(arreglo)
>>> print df
   0  1  2  3
0  1  2  3  4
1  9  9  4  8
>>> print arreglo
[[1 2 3 4]
 [9 9 4 8]]
```
Aunque tienen los mismos elementos, podemos ver que el objeto *df* incluye el nombre de los atributos y renglones. En este caso por defecto los nombres son de nuevo un indice. Más adelante veremos los parámetro *index* y *columns* para cambiar los nombres. Recordemos que en el caso de conjuntos de datos tipo matriz podría no tener mucho sentido el nombre de las columnas ya que no representan atributos de un objeto.

#### Diccionario
Podemos utilizar un diccionario de python para enviar los datos. Podemos utilizar dos variantes:

##### {atributo: secuencia}
En este caso las claves son los nombres de atributos y el valor de cada clave es un vector con los datos correspondientes:

``` python
>>> datos = {'nombre':['ana','tom','joe'], 'email':['ann@example.com','tommy@example.com','jj@example.com']}
>>> df2 = pd.DataFrame(datos)
>>> df2
               email nombre
0    ann@example.com    ana
1  tommy@example.com    tom
2     jj@example.com    joe
```
Como vemos el nombre de los renglones es el indice.
En caso de que pasemos a un solo objeto en lugar de un vector, el valor de este se va a repetir en cada renglón.

``` python
>>> datos = {'nombre':['ana','tom','joe'], 'email':['ann@example.com','tommy@example.com','jj@example.com'],
 'current_state':'active'}
>>> df2 = pd.DataFrame(datos)
>>> df2
  current_state              email nombre
0        active    ann@example.com    ana
1        active  tommy@example.com    tom
2        active     jj@example.com    joe

```

##### {atributo: diccionario}
Si queremos especificar el nombre de cada renglón lo podemos hacer pasando un diccionario por cada atributo, en el diccionario la clave es el nombre del renglón:

``` python
>>>> datos = {'nombre': {'row1':'ana','row2':'tom','row3':'joe'},
        'email':{'row1':'ann@example.com','row2':'tommy@example.com','row3':'jj@example.com'}}
>>> df3 = pd.DataFrame(datos)
>>> print df3
                  email nombre
row1    ann@example.com    ana
row2  tommy@example.com    tom
row3     jj@example.com    joe
```

### dtype

### index
En este parámetro se especifica el índice o nombre de cada renglón.

### columns
En este parámetro se especifica el nombre de cada atributo o columna.

Vamos a pasar ciertos valores para **index** y **columns**:

``` python
>>> datos = [('ana','ann@example.com','active'),
             ('tom','tommy@example.com','active'),
             ('joe','jj@example.com','active')]

>>> nombre_columna = ['nombre','email','current_status']
>>> nombre_renglon = ['r1','r2','r3']
>>> df2 = pd.DataFrame(datos, index=nombre_renglon, columns= nombre_columna)
>>> df2
   nombre              email current_status
r1    ana    ann@example.com         active
r2    tom  tommy@example.com         active
r3    joe     jj@example.com         active
>>>
```

### copy
Cuando enviamos datos utilizando el *numpy.ndarray*, realmente pasamos una referencia a *data*. Esto es más eficiente ya que no se crea una copia adicional. Si lo que queremos es que los datos se copien debemos enviar *True* al parámetro booleano *copy*.


## DataFrame como matriz
¿Podemos utilizar un *DataFrame* para representar una matriz?. Intentemos representar los conjuntos de datos vistos en la sección [Los Datos].

### Relaciones entre objetos

|          | Tijuana  | Ensenada | Mexicali |   
|----------|----------|----------|----------|
| Tijuana  |     0    |   104.2  |  185.5   |    
| Ensenada |  104.2   |       0  |   240.3  |  
| Mexicali |  185.5   |    240.3 |     0    |

Para esta matriz va bien un diccionario:

``` python
In [39]: datos = { 'Tijuana': {'Tijuana':0,'Ensenada':104.2,'Mexicali':185.5}, 'Ensenada': {'Tijuana':104.2,'Ensenada':0,'Mexicali':240.3},'Mexicali': {'Tijuana':185.5,'Ensenada':240.3,'Mexicali':0}}

In [40]: distancias = pd.DataFrame(datos)

In [41]: distancias
Out[41]:
          Ensenada  Mexicali  Tijuana
Ensenada       0.0     240.3    104.2
Mexicali     240.3       0.0    185.5
Tijuana      104.2     185.5      0.0
```
¿Por qué el orden de las ciudades es distinto?

¿Podremos ahorrar espacio o simplificar esta matriz?
Si te fijas la matriz es simétrica, es la misma distancia de Tijuana a Ensenada que de Ensenada a Tijuana. También todas las distancias entre la misma ciudad son obviamente cero.

¿Que tal si eliminamos los casos de la misma ciudad?
``` python
>>> datos = { 'Tijuana': {'Ensenada':104.2,'Mexicali':185.5},
          'Ensenada':{'Tijuana':104.2,'Mexicali':240.3},
          'Mexicali': {'Tijuana':185.5,'Ensenada':240.3}}
```

¿Que tal si eliminamos los casos repetidos?
``` python
>>> datos = { 'Tijuana': {'Ensenada':104.2,'Mexicali':185.5},
          'Ensenada': {'Mexicali':240.3}
        }
```        
Ya que veamos operaciones con matrices, matrices escasas y la biblioteca *NumPy* vamos a regresar a estos ejemplos.

¿Y que tal una matriz para filtrado colaborativo como *DataFrame*?

|     |  The Matrix  | Matando Cabos  | Mad Max: Fury Road |
|-----|--------------|----------------|--------------------|
| Ana |     0        |      1         |  5                 |
| Tom |     4        |      4         |  0                 |
| Joe |     5        |      5         |  5                 |
| Tim |     0        |      1         |  5                 |

Haz esta matriz como ejercicio. Por lo pronto utiliza un diccionario y utiliza una matriz densa, es decir agrega todas las relaciones incluso cuando el valor es cero. Recuerda que el valor de cero en este caso significa que el usuario no ha evaluado la película.

Más adelante veremos como "girar" la matriz para tener en los renglones a las películas y en las columnas a los usuarios.  

### Nota:
Si te fijas, un diccionario nos puede servir perfectamente para almacenar una matriz. En algunos casos incluso será más eficiente y tenemos la ventaja de pasar el diccionario a un *DataFrame* directamente.

# Operaciones básicas del *DataFrame*
Ya tenemos los datos en nuestro *DataFrame*, ¿y ahora?. En esta sección veremos como realizar operaciones básicas de consulta y manipulación de conjuntos de datos.

Vamos a trabajar con los datos de auto-mpg:

``` python
>>> import numpy as np
>>> import pandas as pd
>>> auto_mpg = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s+', header=None,  na_values='?', names=['mpg','cylinders','displacement','horsepower','weight','acceleration','model_year','origin','car_name'], dtype={'mpg':'f4', 'cylinders':'i4',
'displacement':'f4','horsepower':'f4','weight':'f4','acceleration':'f4',
'model_year':'i4','origin':'category','car_name':'category'})
>>> auto_mpg["origin"].cat.categories = ["USA", "Japan", "Germany"]
```
Podemos ver una pequeña muestra de los  **primeros** y **últimos** renglones del conjunto de datos, utilizando los métodos *head()* y *tail()*. En ambos casos el número de registros a mostrar por defecto son 5. Aunque podemos enviar como parámetro el número que deseamos ver. Veamos los tres últimos registros:

``` python
>>> auto_mpg.tail(3)
      mpg  cylinders  displacement  horsepower  weight  acceleration  \
395  32.0          4         135.0        84.0  2295.0          11.6   
396  28.0          4         120.0        79.0  2625.0          18.6   
397  31.0          4         119.0        82.0  2720.0          19.4   

     model_year origin       car_name  
395          82    USA  dodge rampage  
396          82    USA    ford ranger  
397          82    USA     chevy s-10  
```
Mostrar información sobre los indices columnas:
``` python
>>> auto_mpg.index
RangeIndex(start=0, stop=398, step=1)
>>> auto_mpg.columns
Index([u'mpg', u'cylinders', u'displacement', u'horsepower', u'weight',
       u'acceleration', u'model_year', u'origin', u'car_name'],
      dtype='object')
```

Ver los datos almacenado internamente como *numpy.array*:  
``` python      
>>> auto_mpg.values
array([[18.0, 8, 307.0, ..., 70, 'USA', 'chevrolet chevelle malibu'],
       [15.0, 8, 350.0, ..., 70, 'USA', 'buick skylark 320'],
       [18.0, 8, 318.0, ..., 70, 'USA', 'plymouth satellite'],
       ...,
       [32.0, 4, 135.0, ..., 82, 'USA', 'dodge rampage'],
       [28.0, 4, 120.0, ..., 82, 'USA', 'ford ranger'],
       [31.0, 4, 119.0, ..., 82, 'USA', 'chevy s-10']], dtype=object)
>>>
```

Para ordenar por algún atributo se utiliza [*sort_values()*](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html), veamos los primeros registros ordenados por *car_name*:

``` python
>>> auto_mpg.sort_values(by='car_name').head()
           mpg  cylinders  displacement  horsepower  weight  acceleration  \
96   13.000000          8         360.0       175.0  3821.0     11.000000   
9    15.000000          8         390.0       190.0  3850.0      8.500000   
66   17.000000          8         304.0       150.0  3672.0     11.500000   
315  24.299999          4         151.0        90.0  3003.0     20.100000   
257  19.400000          6         232.0        90.0  3210.0     17.200001   

     model_year origin                 car_name  
96           73    USA  amc ambassador brougham  
9            70    USA       amc ambassador dpl  
66           72    USA       amc ambassador sst  
315          80    USA              amc concord  
257          78    USA              amc concord  
```

Se pueden pasar varios atributos al ordenar:

``` python
>>> auto_mpg.sort_values(by=['origin','car_name']).tail()
           mpg  cylinders  displacement  horsepower  weight  acceleration  \
123  20.000000          6         156.0       122.0  2807.0     13.500000   
210  19.000000          6         156.0       108.0  2930.0     15.500000   
343  39.099998          4          79.0        58.0  1755.0     16.900000   
348  37.700001          4          89.0        62.0  2050.0     17.299999   
82   23.000000          4         120.0        97.0  2506.0     14.500000   

     model_year   origin                     car_name  
123          73  Germany               toyota mark ii  
210          76  Germany               toyota mark ii  
343          81  Germany               toyota starlet  
348          81  Germany                toyota tercel  
82           72  Germany  toyouta corona mark ii (sw)  
```

## Seleccionando


## Combinando(Merge)

## Agrupando

## Join estilo SQL
