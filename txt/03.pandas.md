
# Python para Análisis de Datos: Pandas

El análisis de datos comúnmente se realiza sobre datos que se encuentran
organizados en forma de tablas. Probablemente ya haz utilizado datos
organizados de esta manera al utilizar una hoja de cálculo o tablas en
servidores de bases de datos relacionales como Oracle, PostgreSQL o SQLServer.
En python incluso podemos almacenar los datos de una tabla utilizando un
arreglo de tuplas. Primero veamos algunas otras tecnologías que podríamos utilizar:

#### Tablas de SQL

Los sistemas de bases de datos tienen como objetivo almacenar grandes cantidades
de datos y al mismo tiempo nos permiten realizar un gran número de transacciones
de manera concurrente. Dicho de otra manera los servidores nos permiten realizar
la parte operativa de una empresa, vender boletos para conciertos, inscribir
alumnos, llevar el control de un almacén, procesar los pedidos en una tienda en
línea, etc. Además de esto cuentan con un lenguaje de alto nivel para definir,
manipular y consultar a los datos. El lenguaje estándar SQL ha sido el preferido
por los desarrolladores para consultar de una manera muy flexible a los datos
relacionales. El problema que tenemos en estos sistemas es que su objetivo
principal no es el analizar los datos, estos sistemas nos permiten más bien
implementar sistemas que necesitan manipular datos en-línea para funcionar. Por
otro lado, para hacer nuestro análisis, podemos utilizar perfectamente datos que
estén fuera de línea, datos históricos o incluso datos ficticios. Mucho del
*overhead* de un servidor de bases de datos se tiene precisamente por que estos
deben ser capaces de modificar las estructuras de las tablas, hacer
modificaciones a objetos, mantener la consistencia, controlar el acceso y muchas
otras operaciones. Todo en línea. Al no tener estos requisitos podemos operar
con estructuras diseñadas precisamente para el análisis de datos.  Dicho esto,
algunos sistemas de bases de datos han agregado la funcionalidad necesaria para
realizar el tipo de análisis que veremos, pero esto no es todavía el estándar.
Los sistemas de bases de datos nos pueden servir para almacenar los datos que
utilizaremos en el análisis y realizar algunas consultas con SQL. Podemos
utilizar también la biblioteca SQLite para almacenar bitácoras, intercambiar
información con otros sistemas o como parte del preprocesamiento.

#### NoSQL

Últimamente se han propuesto gestores de datos que aunque no siguen el modelo
relacional nos permiten hacer cierto tipo de consultas de una manera mucho más
eficiente. Por ejemplo, los almacenes clave-valor (key-value) pueden recuperar
bastante rápido los datos por medio de su clave; pero el lenguaje de consulta,
si es que lo hay, no es muy flexible. Son muy buenos para recuperar muy rápido
un documento html o un objeto con solo especificar su clave, pero no podemos
hacer consultas *ad-hoc* como "recupera aquellos productos que tengan ventas
superiores al promedio en las tiendas de California o Arizona, siempre que no
sean del departamento de electrónica". Estos sistemas nos pueden servir sobre
todo como memoria cache o memoria compartida para realizar operaciones en
paralelo.

#### Data Warehouse

Las técnicas de Almacenes de Datos (Data Warehouse) nos permiten realizar de una
manera muy sencilla consultas *tipo cubo* sobre bases de datos relacionales.
Aunque las consultas permiten a los ejecutivos tomar decisiones y tener una
buena idea de lo que sucede en la empresa, su objetivo no es el de extraer
patrones de los datos. Sin embargo el objetivo general y el diseño de los
Almacenes de Datos tiene algunas cosas en común con el proceso de KDD. Ambos
trabajan con datos históricos y requieren de un proceso previo de extracción de
los datos al cual se le conoce como ETL (Extract, Transform and Load) o
Extraer, Transformar y Cargar). El proceso de ETL permite a las empresas extraer
datos desde diferentes fuentes, cambiar de formato  y limpiarlos con el objetivo
de  cargarlos en el almacén de datos.

#### Hojas de Cálculo

De hecho las hojas de cálculo son una herramienta aceptable para realizar
análisis de datos. Muchos analistas utilizan hojas de calculo como herramienta
principal. No se requiere tener un amplío conocimiento de programación para
sacarles provecho y pueden complementares con otras herramientas por ejemplo con
sistemas de bases de datos. Los programadores normalmente preferimos tener un
control total sobre el proceso de KDD y nos gusta poder integrar fácilmente
nuestro código en distintas aplicaciones, por lo que preferimos la flexibilidad
que nos brinda un lenguaje de propósito general.

## Breve Introducción al DataFrame de Pandas

Pandas es una biblioteca código abierto en Python la cual se ha diseñado con el objetivo específico de brindarnos una estructura de tabla rápida y flexible para el análisis de datos. Según
su [documentación](http://pandas.pydata.org/pandas-docs/stable/)
pandas nos permite operar con distintos tipos de datos:

* Tablas con atributos heterogéneos.
* Series de tiempo, ordenadas o no.
* Matrices homogéneas o heterogéneas.

Para esto en pandas se implementan dos estructuras principales: *Series* y *DataFrame*. La estructura *Series* nos permite trabajar con datos de series de tiempo especificadas como un vector. Por otro lado está *DataFrame* nos sirve para aquellos datos que tienen una estructura de tabla o matriz. En esta sección nos enfocaremos en la estructura *DataFrame* ya que nos interesa trabajar con datos organizados como tabla.

### Creando un DataFrame desde un archivo de texto

Como ejemplo, vamos a utilizar una estructura tipo DataFrame para almacenar el conjunto de
datos conocido como [Auto
MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG). Lo primero que debemos  hacer es ver los tipos de datos de los atributos:

|  nombre        | tipo     | medida     |  descripción             |
|----------------|----------| -----------|--------------------------|
|  mpg           | continuo | razón      | millas por galón         |
|  cylinders     | discreto | ordinal    | número de cilindros      |
|  displacement  | continuo | razón      | desplazamiento           |
|  horsepower    | continuo | razón      | caballos de fuerza       |
|  weight        | continuo | razón      | peso en libras US        |
|  acceleration  | continuo | razón      | aceleración              |
|  model_year    | discreto | razón      | año de fabricación       |
|  origin        | discreto | categórico | origen                   |
|  car_name      | cadena   | categórico | nombre único del auto    |

Como vemos estas observaciones tienen atributos heterogéneos ya que son de distintos tipos. El objetivo original de este conjunto de datos era el de predecir el consumo de combustible en millas por galón (mpg) utilizando los otros atributos.

Como siguiente paso vamos a descargar el conjunto de datos del [repositorio de
machine learning de la
UC-Irvine](https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/).
El archivo se llama **auto-mpg.data**. Al abrir el archivo en un editor de texto
vemos que está separado por espacios y faltan algunos valores. Veamos un
fragmento:

```python
11.0   8   350.0      180.0      3664.      11.0   73  1	"oldsmobile omega"
20.0   6   198.0      95.00      3102.      16.5   74  1	"plymouth duster"
21.0   6   200.0      ?          2875.      17.0   74  1	"ford maverick"
```

Pandas nos brinda *IO Tools* un API de entrada/salida donde se incluyen métodos
de lectura para formatos de texto, binarios y SQL. En el caso de text se
incluyen lectores para formatos CSV, JSON y HTML. Vamos a intentar leer el
archivo utilizando el método *read_csv()* para más detalle puedes ver la
[documentación](http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table).
La manera en la que se lee y se hace el "parsing" a los archivos se puede configurar de una manera muy detallada, para este ejemplo solo modificaremos algunos parámetros básicos. Recordemos que en los archivos CSV, como el nombre lo dice, los valores de los atributos se separan por comas. Ya vimos que en el  caso del archivo **auto-mpg.data**  la separación se hace por medio de espacios en blanco. Un problema adicional es que el número de espacios no siempre es el mismo. Para leer el archivo correctamente, vamos a utilizar el parámetro *sep* el cual recibe una especificación del separador que se va a utilizar, por defecto una coma ','. En nuestro caso el separador será una expresión regular la cual le diga al método que son uno o más espacios. El espacio y tabuladores se especifican en la [expresión regular](https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular) con la cadena **'\\s'**, mientras que el operador **'+'** indica que se puede repetir una o más veces. Nuestro primer intento quedaría de la siguiente manera:

``` python
>>> import matplotlib.pyplot as plt
>>> import pandas as pd
>>> import numpy as np
>>> df = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s+')
```

Antes que nada importamos las bibliotecas que utilizaremos en estos
ejercicios (1-3). Como primer parámetro enviamos la ruta al archivo *auto-mpg.data*, si ejecutamos el comando dentro del directorio del libro  solamente especificamos la ruta de la siguiente manera: *'datos-ejemplo/auto-mpg.data'*. En caso de que se encuentre en otro lado simplemente cambiamos la ruta.

#### Nota:

En caso de que te marque algún error, tal vez debas actualizar las versiones de las bibliotecas de anaconda. Desde la línea de comando de tu sistema operativo ejecuta este comando:

```
conda update --all
```

``` python
>>> df
     18.0  8  307.0  130.0   3504.  12.0  70  1  \
0    15.0  8  350.0  165.0  3693.0  11.5  70  1   
1    18.0  8  318.0  150.0  3436.0  11.0  70  1   
2    16.0  8  304.0  150.0  3433.0  12.0  70  
..
chevrolet chevelle malibu  
0                    buick skylark 320  
1                   plymouth satellite  
2                        amc rebel sst  
3                          ford torino  


[397 rows x 9 columns]
```

El método lector espera que el primer renglón tenga el nombre de los atributos, pero nuestro archivo no los tiene. Por lo que el nombre de los atributos no tienen mucho sentido *"18.0  8  307.0  130.0   3504.  12.0  70  1 chevrolet chevelle malibu"*. Debemos indicar que nuestro archivo no incluye el renglón de encabezados con el argumento *header=None*. En este caso, como no hemos especificado nombres aun los atributos tendrán como nombre simplemente un indice.

Vamos a corregir el problema indicando que no se tiene una línea de encabezado:

``` python
>>> df = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s+', header=None)

>>> df
        0  1      2      3       4     5   6  7  \
0    18.0  8  307.0  130.0  3504.0  12.0  70  1   
1    15.0  8  350.0  165.0  3693.0  11.5  70  1   
..
8  
0            chevrolet chevelle malibu  
1                    buick skylark 320
..

[398 rows x 9 columns]
```

Vamos especificando los nombres de los atributos, esto se hace agregando el parámetro **names** y enviando una lista con los nombres:

``` python
>>> df2 = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s+', header=None,
 names=['mpg','cylinders','displacement','horsepower','weight','acceleration',
 'model_year','origin','car_name'])

>>> df2
      mpg  cylinders  displacement horsepower  weight  acceleration  \
0    18.0          8         307.0      130.0  3504.0          12.0   
1    15.0          8         350.0      165.0  3693.0          11.5   
2    18.0          8         318.0      150.0  3436.0          11.0   
```
Mucho mejor, veamos que tipo de datos infirió el método de lectura para cada una de las columnas:

``` python
>>> df2.dtypes

mpg             float64
cylinders         int64
displacement    float64
horsepower       object
weight          float64
acceleration    float64
model_year        int64
origin            int64
car_name         object
dtype: object

```
Hay un problema con el atributo *horsepower* este debería de ser float64. Lo que
sucede es que al leer el carácter *?* el método de lectura no sabe que tipo de
dato inferir. Lo bueno es que podemos configurar como queremos que se resuelvan
los casos donde tenemos datos "no disponibles" y también como se debe pasar este tipo de valores. El argumento que vamos a utilizar es *na_values*, podemos indicar una
lista de valores o como en este caso simplemente el símbolo correspondiente.

``` python
>>> df2 = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s+', header=None,
         names=['mpg','cylinders','displacement','horsepower','weight',
         'acceleration','model_year','origin','car_name'], na_values='?')

In [12]: df2.dtypes
Out[12]:
mpg             float64
cylinders         int64
displacement    float64
horsepower      float64
weight          float64
acceleration    float64
model_year        int64
origin            int64
car_name         object
dtype: object
```
Ahora si, cada una de las columnas corresponde al tipo de dato de los atributos de la tabla anterior. Si queremos también podemos indicar al momento de la lectura el nombre y tipo de dato de los atributos. Incluso podemos indicar si tenemos algún dato categórico.

Mantener origin como entero

``` python
>>> df2 = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s+',
     header=None,  na_values='?', names=['mpg','cylinders','displacement','horsepower',
        'weight','acceleration','model_year','origin','car_name'],
     dtype={'mpg':'f4','cylinders':'i4','displacement':'f4',
        'horsepower':'f4','weight':'f4','acceleration':'f4',
        'model_year':'i4','origin':'i4','car_name':'category'})

>>> df2.dtypes

mpg              float32
cylinders          int32
displacement     float32
horsepower       float32
weight           float32
acceleration     float32
model_year         int32
origin             int32
car_name        category
dtype: object
```
Una vez leído correctamente el *DataFrame* podemos utilizar el método **describe()** para generar un resumen estadístico descriptivo que incluye la tendencia central, dispersión y la distribución del conjunto de datos, excluyendo datos categóricos y valores no disponibles.

``` python
>>> df2.describe()

              mpg   cylinders  displacement  horsepower       weight  \
count  398.000000  398.000000    398.000000  392.000000   398.000000   
mean    23.514574    5.454774    193.425873  104.469391  2970.424561   
std      7.815985    1.701004    104.269859   38.491138   846.841431   
min      9.000000    3.000000     68.000000   46.000000  1613.000000   
25%     17.500000    4.000000    104.250000   75.000000  2223.750000   
50%     23.000000    4.000000    148.500000   93.500000  2803.500000   
75%     29.000000    8.000000    262.000000  126.000000  3608.000000   
max     46.599998    8.000000    455.000000  230.000000  5140.000000   

       acceleration  model_year  
count    398.000000  398.000000  
mean      15.568086   76.010050  
std        2.757689    3.697627  
min        8.000000   70.000000  
25%       13.825000   73.000000  
50%       15.500000   76.000000  
75%       17.175001   79.000000  
max       24.799999   82.000000  
```

Ya que hablamos de los datos categóricos, vamos especificando el nombre de cada categoría. En el archivo solo se especifican los valores 1, 2 y 3. Al ver los modelos de los autos, inferimos que deben ser "USA", "Japan" y "Germany".
Mapea la columna 'origin' con los valores correspondientes, asegurarte de que la columna 'origin' sea tipo 'category' y establecer las categorías.

``` python
>>> origin_map = {1: 'USA', 2: 'Japan', 3: 'Germany'}
>>> df2['origin'] = df2['origin'].map(origin_map)
>>> df2['origin'] = df2['origin'].astype('category')

>>> df2['origin'] = df2['origin'].cat.set_categories(["USA", "Japan", "Germany"])
>>> df2['origin']

0          USA
1          USA
2          USA
3          USA
4          USA
..
390    Germany
391        USA
392        USA
393        USA
394      Japan
395        USA
396        USA
397        USA

Name: origin, Length: 398, dtype: category
Categories (3, object): [USA, Japan, Germany]
```
¡Muy bien!, ¿ahora que tal si hacemos una gráfica de pastel?. Para esto generamos la gráfica con el método **plot()**. Todo esto lo veremos a detalle en la sección de visualización de datos.    

``` python
>>> df2['origin'].value_counts().plot(kind='bar')
<matplotlib.axes._subplots.AxesSubplot at 0x10d1d1cd0>
```
Una vez creada la gráfica la mostramos con el método **show()** de la biblioteca *matplotlib*.

``` python
>>> plt.show()
```
Deberíamos ver la siguiente gráfica:

![pie](../img/pie.png)

#### Nota:
Debes de cerrar la ventana donde se muestra la gráfica para desbloquear el interprete y poder continuar. Puedes grabar la gráfica si gustas.

Vamos ahora a graficar tres variables para ver si vemos algo interesante:

``` python
>>> df2.plot.scatter(x='weight', y='mpg',c='horsepower',cmap='viridis');
>>> plt.show()
```

![Plot con mapa de colores](../img/cmap.png)

Intenta cambiar el mapa de colores  con otras opciones [Color Maps](http://matplotlib.org/examples/color/colormaps_reference.html).

Si quieres intentar algunas otras gráficas adelante:
http://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization

Como hemos visto una vez que tenemos nuestros datos en la estructura *DataFrame* podemos hacer cosas interesantes muy fácilmente.

En esta sección vimos un ejemplo sencillo de como leer los datos de un archivo ya que es una tarea muy común. Ahora veremos como almacenar los datos utilizando objetos de python ya que esto nos brindará una flexibilidad adicional, por ejemplo para leer los datos desde una base de datos No-SQL o por http.

### Creando un DataFrame con el constructor

El constructor de la clase  [DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) puede tomar los siguientes parámetros: **data**, **index**, **columns**, **dtype** y **copy**. Vemos cada uno de ellos:

#### data

En este parámetro enviamos los datos, como sucede muchas veces en Python, estos pueden ser de varios tipos:

#### lista de tuplas

La manera más sencilla de enviar datos a un *DataFrame* es utilizando una lista de tuplas:

``` python
>>> import matplotlib.pyplot as plt
>>> import pandas as pd
>>> import numpy as np
>>> datos = [('ana','ann@example.com','active'),
             ('tom','tommy@example.com','active'),
             ('joe','jj@example.com','active')]
>>> df2 = pd.DataFrame(datos)
>>> df2
     0                  1       2
0  ana    ann@example.com  active
1  tom  tommy@example.com  active
2  joe     jj@example.com  active
>>>
```
La lista de tuplas puede venir de una consulta de base de datos u otro proceso.

#### numpy.ndarray

Los datos pueden venir también como un arreglo *n-dimensional* de la biblioteca [numpy](https://docs.scipy.org/doc/numpy/user/index.html). De hecho la estructura *DataFrame* utiliza internamente este tipo de objetos. Un *ndarray* es un contenedor multidimensional de objetos del mismo tipo y tamaño. Los arreglos multidimensionales también cuentan con un objeto de definición de tipo (data-type dtype).

Como ejemplo vamos a crear un arreglo *dimensión-2*, de tamaño 2x4 el cual contiene enteros de 32 bits:

``` python
>>> arreglo = np.array([[1, 2, 3, 4], [9, 9, 4, 8]], dtype=np.int32)
>>> type(arreglo)
<type 'numpy.ndarray'>
>>> arreglo.shape
(2, 4)
>>> arreglo.dtype
dtype('int32')
```

Podemos revisar el tamaño y tipo de los objetos *numpy.ndarray* con los atributos *shape* y *dtype* respectivamente. Si te fijas la tupla que regresa el atributo *shape* nos dice el número de renglones primero y después el número de columnas.  

Ahora vamos a crear un *DataFrame* a partir de nuestro arreglo:
``` python
>>> df = pd.DataFrame(arreglo)
>>> print df
   0  1  2  3
0  1  2  3  4
1  9  9  4  8
>>> print arreglo
[[1 2 3 4]
 [9 9 4 8]]
```

Aunque tienen los mismos elementos, podemos ver que el objeto *df* incluye el nombre de los atributos y renglones. En este caso por defecto los nombres son de nuevo un indice. Más adelante veremos los parámetro *index* y *columns* para cambiar los nombres. Recordemos que en el caso de conjuntos de datos tipo matriz podría no tener mucho sentido el nombre de las columnas ya que no representan atributos de un objeto.

#### Diccionario

Podemos utilizar un diccionario de python para enviar los datos. Podemos utilizar dos variantes:

##### {atributo: secuencia}

En este caso las claves son los nombres de atributos y el valor de cada clave es un vector con los datos correspondientes:

``` python
>>> datos = {'nombre':['ana','tom','joe'],
    'email':['ann@example.com','tommy@example.com','jj@example.com']}
>>> df2 = pd.DataFrame(datos)
>>> df2
               email nombre
0    ann@example.com    ana
1  tommy@example.com    tom
2     jj@example.com    joe
```
Como vemos el nombre de los renglones es el indice.
En caso de que pasemos a un solo objeto en lugar de un vector, el valor de este se va a repetir en cada renglón.

``` python
>>> datos = {'nombre':['ana','tom','joe'],
  'email':['ann@example.com','tommy@example.com','jj@example.com'],
  'current_state':'active'}
>>> df2 = pd.DataFrame(datos)
>>> df2
  current_state              email nombre
0        active    ann@example.com    ana
1        active  tommy@example.com    tom
2        active     jj@example.com    joe

```

#### {atributo: diccionario}

Si queremos especificar el nombre de cada renglón lo podemos hacer pasando un diccionario por cada atributo, en el diccionario la clave es el nombre del renglón:

``` python
>>>> datos = {
        'nombre': {
                 'row1':'ana',
                 'row2':'tom',
                 'row3':'joe'},
        'email':{'row1':'ann@example.com',
                 'row2':'tommy@example.com',
                 'row3':'jj@example.com'}}
>>> df3 = pd.DataFrame(datos)
>>> print df3
                  email nombre
row1    ann@example.com    ana
row2  tommy@example.com    tom
row3     jj@example.com    joe
```

#### index

En este parámetro se especifica el índice o nombre de cada renglón.

#### columns

En este parámetro se especifica el nombre de cada atributo o columna.

Vamos a pasar ciertos valores para **index** y **columns**:

``` python
>>> datos = [('ana','ann@example.com','active'),
             ('tom','tommy@example.com','active'),
             ('joe','jj@example.com','active')]

>>> nombre_columna = ['nombre','email','current_status']
>>> nombre_renglon = ['r1','r2','r3']
>>> df2 = pd.DataFrame(datos, index=nombre_renglon, columns= nombre_columna)
>>> df2
   nombre              email current_status
r1    ana    ann@example.com         active
r2    tom  tommy@example.com         active
r3    joe     jj@example.com         active
>>>
```

#### copy

Cuando enviamos datos utilizando el *numpy.ndarray*, realmente pasamos una referencia a *data*. Esto es más eficiente ya que no se crea una copia adicional. Si lo que queremos es que los datos se copien debemos enviar *True* al parámetro booleano *copy*.

## DataFrame como matriz
¿Podemos utilizar un *DataFrame* para representar una matriz?. Intentemos representar los conjuntos de datos vistos en la sección [Los Datos].

### Relaciones entre objetos

|          | Tijuana  | Ensenada | Mexicali |   
|----------|----------|----------|----------|
| Tijuana  |     0    |   104.2  |  185.5   |    
| Ensenada |  104.2   |       0  |   240.3  |  
| Mexicali |  185.5   |    240.3 |     0    |

Para esta matriz va bien un diccionario:

``` python
>>> datos = {
  'Tijuana': {'Tijuana':0,'Ensenada':104.2,'Mexicali':185.5},
  'Ensenada': {'Tijuana':104.2,'Ensenada':0,'Mexicali':240.3},
  'Mexicali': {'Tijuana':185.5,'Ensenada':240.3,'Mexicali':0}
   }

>>> distancias = pd.DataFrame(datos)

>>> distancias

          Ensenada  Mexicali  Tijuana
Ensenada       0.0     240.3    104.2
Mexicali     240.3       0.0    185.5
Tijuana      104.2     185.5      0.0
```
¿Por qué el orden de las ciudades es distinto?

¿Podremos ahorrar espacio o simplificar esta matriz?
Si te fijas la matriz es simétrica, es la misma distancia de Tijuana a Ensenada que de Ensenada a Tijuana. También todas las distancias entre la misma ciudad son obviamente cero.

¿Que tal si eliminamos los casos de la misma ciudad?
``` python
>>> datos = { 'Tijuana':{'Ensenada':104.2,'Mexicali':185.5},
              'Ensenada':{'Tijuana':104.2,'Mexicali':240.3},
              'Mexicali':{'Tijuana':185.5,'Ensenada':240.3}}
```

¿Que tal si eliminamos los casos repetidos?
``` python
>>> datos = { 'Tijuana': {'Ensenada':104.2,'Mexicali':185.5},
          'Ensenada': {'Mexicali':240.3}
        }
```

Ya que veamos operaciones con matrices, matrices escasas y la biblioteca *NumPy* vamos a regresar a estos ejemplos.

¿Y que tal una matriz para filtrado colaborativo como *DataFrame*?

|     |  The Matrix  | Matando Cabos  | Mad Max: Fury Road |
|-----|--------------|----------------|--------------------|
| Ana |     0        |      1         |  5                 |
| Tom |     4        |      4         |  0                 |
| Joe |     5        |      5         |  5                 |
| Tim |     0        |      1         |  5                 |

Haz esta matriz como ejercicio. Por lo pronto utiliza un diccionario y utiliza una matriz densa, es decir agrega todas las relaciones incluso cuando el valor es cero. Recuerda que el valor de cero en este caso significa que el usuario no ha evaluado la película.

Más adelante veremos como "girar" la matriz para tener en los renglones a las películas y en las columnas a los usuarios.  

##### Nota:

Si te fijas, un diccionario nos puede servir perfectamente para almacenar una matriz. En algunos casos incluso será más eficiente y tenemos la ventaja de pasar el diccionario a un *DataFrame* directamente.

## Operaciones básicas del *DataFrame*

Ya tenemos los datos en nuestro *DataFrame*, ¿y ahora?. En esta sección veremos como realizar operaciones básicas de consulta y manipulación de conjuntos de datos.

Vamos a trabajar con los datos de auto-mpg:

``` python
>>> import numpy as np
>>> import pandas as pd
>>> auto_mpg = pd.read_csv('datos-ejemplo/auto-mpg.data', sep='\s+',
     header=None,  na_values='?', names=['mpg','cylinders','displacement','horsepower',
        'weight','acceleration','model_year','origin','car_name'],
     dtype={'mpg':'f4', 'cylinders':'i4',
        'displacement':'f4','horsepower':'f4','weight':'f4',
        'acceleration':'f4','model_year':'i4','origin':'category',
        'car_name':'category'})

>>> auto_mpg["origin"].cat.categories = ["USA", "Japan", "Germany"]

```

Podemos ver una pequeña muestra de los  **primeros** y **últimos** renglones del conjunto de datos, utilizando los métodos *head()* y *tail()*. En ambos casos el número de registros a mostrar por defecto son 5. Aunque podemos enviar como parámetro el número que deseamos ver. Veamos los tres últimos registros:

``` python
>>> auto_mpg.tail(3)
      mpg  cylinders  displacement  horsepower  weight  acceleration  \
395  32.0          4         135.0        84.0  2295.0          11.6   
396  28.0          4         120.0        79.0  2625.0          18.6   
397  31.0          4         119.0        82.0  2720.0          19.4   

     model_year origin       car_name  
395          82    USA  dodge rampage  
396          82    USA    ford ranger  
397          82    USA     chevy s-10  
```

Mostrar información sobre los *indices* o nombres de renglón y las columnas:

``` python
>>> auto_mpg.index
RangeIndex(start=0, stop=398, step=1)
>>> auto_mpg.columns
Index([u'mpg', u'cylinders', u'displacement', u'horsepower', u'weight',
       u'acceleration', u'model_year', u'origin', u'car_name'],
      dtype='object')
```

Ver los datos almacenado internamente como *numpy.array*:

``` python      
>>> auto_mpg.values
array([[18.0, 8, 307.0, ..., 70, 'USA', 'chevrolet chevelle malibu'],
       [15.0, 8, 350.0, ..., 70, 'USA', 'buick skylark 320'],
       [18.0, 8, 318.0, ..., 70, 'USA', 'plymouth satellite'],
       ...,
       [32.0, 4, 135.0, ..., 82, 'USA', 'dodge rampage'],
       [28.0, 4, 120.0, ..., 82, 'USA', 'ford ranger'],
       [31.0, 4, 119.0, ..., 82, 'USA', 'chevy s-10']], dtype=object)
>>>
```

Para ordenar por algún atributo se utiliza [*sort_values()*](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html), veamos los primeros registros ordenados por *car_name*:

``` python
>>> auto_mpg.sort_values(by='car_name').head()
           mpg  cylinders  displacement  horsepower  weight  acceleration  \
96   13.000000          8         360.0       175.0  3821.0     11.000000   
9    15.000000          8         390.0       190.0  3850.0      8.500000   
66   17.000000          8         304.0       150.0  3672.0     11.500000   
315  24.299999          4         151.0        90.0  3003.0     20.100000   
257  19.400000          6         232.0        90.0  3210.0     17.200001   

     model_year origin                 car_name  
96           73    USA  amc ambassador brougham  
9            70    USA       amc ambassador dpl  
66           72    USA       amc ambassador sst  
315          80    USA              amc concord  
257          78    USA              amc concord  
```

Se pueden pasar varios atributos al ordenar:

``` python
>>> auto_mpg.sort_values(by=['origin','car_name']).tail()
           mpg  cylinders  displacement  horsepower  weight  acceleration  \
123  20.000000          6         156.0       122.0  2807.0     13.500000   
210  19.000000          6         156.0       108.0  2930.0     15.500000   
343  39.099998          4          79.0        58.0  1755.0     16.900000   
348  37.700001          4          89.0        62.0  2050.0     17.299999   
82   23.000000          4         120.0        97.0  2506.0     14.500000   

     model_year   origin                     car_name  
123          73  Germany               toyota mark ii  
210          76  Germany               toyota mark ii  
343          81  Germany               toyota starlet  
348          81  Germany                toyota tercel  
82           72  Germany  toyouta corona mark ii (sw)  
```

### Seleccionando

Podemos utilizar tajadas (*slicing*) sobre los renglones:

``` python
>>> auto_mpg[2:5]
    mpg  cylinders  displacement  horsepower  weight  acceleration  \
2  18.0          8         318.0       150.0  3436.0          11.0   
3  16.0          8         304.0       150.0  3433.0          12.0   
4  17.0          8         302.0       140.0  3449.0          10.5   
```

También podemos utilizar el nombre de la columna como un atributo del objeto *DataFrame*.

``` python
>>> auto_mpg.car_name[:3]
0    chevrolet chevelle malibu
1            buick skylark 320
2           plymouth satellite
Name: car_name, dtype: category
```

Podemos pasar también una lista de atributos:

``` python
>>> auto_mpg[['car_name','origin', 'model_year' ] ].head()
                    car_name origin  model_year
0  chevrolet chevelle malibu    USA          70
1          buick skylark 320    USA          70
2         plymouth satellite    USA          70
3              amc rebel sst    USA          70
4                ford torino    USA          70
```

Se recomienda utilizar el atributo *loc* para hacer slicing indexado por las etiquetas. Veamos un ejemplo:

``` python
>>> auto_mpg.loc[3:5,'mpg':'weight']
    mpg  cylinders  displacement  horsepower  weight
3  16.0          8         304.0       150.0  3433.0
4  17.0          8         302.0       140.0  3449.0
5  15.0          8         429.0       198.0  4341.0
```

Es importante ver que la tajada difiere de los cortes de Python pues el resultado abarca ambas etiquetas.

También se puede pasar una lista de las etiquetas que requerimos:

``` python
>>> auto_mpg.loc[[3,2,5],'mpg':'weight']
    mpg  cylinders  displacement  horsepower  weight
3  16.0          8         304.0       150.0  3433.0
2  18.0          8         318.0       150.0  3436.0
5  15.0          8         429.0       198.0  4341.0
```

Si enviamos una lista binaria, se hace un *match* con solo las posiciones que tienen valor verdadero:

``` python
>>> auto_mpg.loc[[0,12,167,234], [True,True,False,False,False,True,False,True,True]].head()
      mpg  cylinders  acceleration   origin                   car_name
0    18.0          8          12.0      USA  chevrolet chevelle malibu
12   15.0          8           9.5      USA      chevrolet monte carlo
167  29.0          4          16.0  Germany             toyota corolla
234  24.5          4          16.0      USA      pontiac sunbird coupe
```

Para cortes basados en la posición se utiliza *iloc*:

``` python
>>> auto_mpg.iloc[0:2,0:2]
    mpg  cylinders
0  18.0          8
1  15.0          8
```

En este caso los cortes funcionan igual que en Python.

Un aspecto poderoso de estas estructuras es que podemos realizar operaciones tipo *map()* de una manera muy sencilla. Por ejemplo, para calcular el doble de  la columna *mpg* y regresar una nueva lista se haría lo siguiente:

``` python
>>> auto_mpg.mpg.head() * 2
0    36.0
1    30.0
2    36.0
3    32.0
4    34.0
```

Esta operación no se podría hacer en datos categóricos:

``` python
>>> auto_mpg.origin.head() * 2
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/mariosky/anaconda/lib/python2.7/site-packages/pandas/core/ops.py", line 721, in wrapper
    result = wrap_results(safe_na_op(lvalues, rvalues))
  File "/Users/mariosky/anaconda/lib/python2.7/site-packages/pandas/core/ops.py", line 682, in safe_na_op
    return na_op(lvalues, rvalues)
  File "/Users/mariosky/anaconda/lib/python2.7/site-packages/pandas/core/ops.py", line 672, in na_op
    op=str_rep))
TypeError: Categorical cannot perform the operation *
```

En los datos categóricos podemos utilizar métodos de cadenas mediante el atributo *str*:

``` python
>>> auto_mpg.car_name.str.upper().iloc[:5]
0    CHEVROLET CHEVELLE MALIBU
1            BUICK SKYLARK 320
2           PLYMOUTH SATELLITE
3                AMC REBEL SST
4                  FORD TORINO
Name: car_name, dtype: object
```

Vamos a utilizar varios conceptos vistos hasta este momento para resolver un problema. Vamos a comprar un auto, pero queremos que tenga un rendimiento superior a 40 mpg.  

Como primer paso vamos a generar una lista binaria con aquellos autos que cumplen con la condición:

``` python
>>> auto_mpg.mpg > 40
0      False
1      False
       ...  
393    False
394     True
395    False
396    False
397    False
Name: mpg, Length: 398, dtype: bool
```

Ahora vamos a realizar un corte a partir de la lista binaria como lo habíamos hecho antes y vamos a mostrar solo cuatro atributos:

``` python
>>> auto_mpg[auto_mpg.mpg > 40 ].loc[ :, ['mpg','model_year','origin','car_name']]
           mpg  model_year   origin                         car_name
244  43.099998          78    Japan  volkswagen rabbit custom diesel
309  41.500000          80    Japan                        vw rabbit
322  46.599998          80  Germany                        mazda glc
324  40.799999          80  Germany                       datsun 210
325  44.299999          80    Japan             vw rabbit c (diesel)
326  43.400002          80    Japan               vw dasher (diesel)
329  44.599998          80  Germany              honda civic 1500 gl
330  40.900002          80    Japan             renault lecar deluxe
394  44.000000          82    Japan                        vw pickup
```

Podemos ver que solo autos de Japón y Alemania cumplen con la condición.

Si queremos filtrar por datos categóricos podemos utilizar comprensión de listas. Por ejemplo, para ver solo aquellos autos de Japón:

``` python
>>> auto_mpg[[ auto in ['Japan']  for auto in  auto_mpg.origin]].loc[19:55,['mpg','model_year','origin','car_name']]
     mpg  model_year origin                      car_name
19  26.0          70  Japan  volkswagen 1131 deluxe sedan
20  25.0          70  Japan                   peugeot 504
21  24.0          70  Japan                   audi 100 ls
22  25.0          70  Japan                      saab 99e
23  26.0          70  Japan                      bmw 2002
50  28.0          71  Japan                     opel 1900
51  30.0          71  Japan                   peugeot 304
52  30.0          71  Japan                     fiat 124b
55  27.0          71  Japan          volkswagen model 111
```

Un detalle de este ejemplo es que al utilizar el corte por etiqueta *loc* los valores que se ponen en la especificación del rango *19:55* corresponden a la etiqueta no a la posición. Si hubiéramos puesto 0:5 para obtener los primeros 5 valores realmente nos regresaría un DataFrame vacío. Para utilizar ese estilo debemos de utilizar el atributo *iloc*.

### Concatenar

Primeramente vamos a extraer a los autos de Japón y Alemania:
``` python
>>> japon = auto_mpg[[ auto in ['Japan']  for auto in  auto_mpg.origin]]
>>> alemania = auto_mpg[[ auto in ['Germany']  for auto in  auto_mpg.origin]]

```
Ahora vamos a crear un nuevo *DataFrame* con ambos conjuntos de datos:

``` python
>>> non_usa = pd.concat([japon, alemania])
>>> non_usa.loc[:, ['mpg','model_year','origin','car_name']]

           mpg  model_year   origin                      car_name
19   26.000000          70    Japan  volkswagen 1131 deluxe sedan
20   25.000000          70    Japan                   peugeot 504
21   24.000000          70    Japan                   audi 100 ls
22   25.000000          70    Japan                      saab 99e
23   26.000000          70    Japan                      bmw 2002
50   28.000000          71    Japan                     opel 1900
51   30.000000          71    Japan                   peugeot 304
52   30.000000          71    Japan                     fiat 124b

...

380  36.000000          82  Germany              nissan stanza xe
381  36.000000          82  Germany                  honda accord
382  34.000000          82  Germany                toyota corolla
383  38.000000          82  Germany                   honda civic
384  32.000000          82  Germany            honda civic (auto)
385  38.000000          82  Germany                 datsun 310 gx
390  32.000000          82  Germany              toyota celica gt

[149 rows x 4 columns]
```

### Join estilo SQL

Aunque es preferible hacer estas operaciones en el sistema de base de datos, es posible hacer *Joins* entre estructuras *DataFrame*. Como ejemplo vamos a crear dos DataFrames para empleados y sus puestos.


``` python
>>> empleado = {'nombre':['ana','tom','joe'],
                'email':['ann@example.com','tommy@example.com',
                          'jj@example.com'],
                'puesto':[1,1,2] }

>>> emplado_df = pd.DataFrame(empleado)
>>> emplado_df
>>> emplado_df
   departamento              email nombre
0             1    ann@example.com    ana
1             1  tommy@example.com    tom
2             2     jj@example.com    joe

>>> puesto = {'pid':[1,2,3],
             'puesto':['DBA','ML','DQ'] }
>>> puesto_df = pd.DataFrame(puesto)
>>> puesto_df
   pid puesto
0    1    DBA
1    2     ML
2    3     DQ            

```

Ahora vamos a hacer un *Join* para desplegar los datos del empleado y su departamento:

``` python
>>> pd.merge(empleado_df, puesto_df, left_on='puesto', right_on='pid')

               email nombre  puesto_x  pid puesto_y
0    ann@example.com    ana         1    1      DBA
1  tommy@example.com    tom         1    1      DBA
2     jj@example.com    joe         2    2       ML
```

Para otros tipos de Join [la documentación de pandas](http://pandas.pydata.org/pandas-docs/stable/merging.html#merging-join) es una buena referencia.

### Agrupando

``` python
>>> auto_mpg.groupby('origin').count().loc[:, 'mpg']
origin
USA        249
Japan       70
Germany     79
```

## Resumen

Pandas es una biblioteca para el análisis de datos en Python, que tiene como principales estructuras a *Series* y *DataFrame*. La estructura *Series* nos permite procesar datos en dimensión-1 para series de tiempo y vectores. Para los conjuntos de datos en forma tabular y de matriz de n-dimensiones se utiliza el *DataFrame*. Debes de saber como crear objetos tipo *DataFrame* ya sea leyendo de un archivo de texto o desde código en Python. También debes conocer como crear matrices para distintas aplicaciones.

## Ejercicios

#### iris.data

1. Lee desde archivo el conjunto de datos [iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/) utiliza el método **read_csv()**.

2. Nombra a los atributos como se especifica en la información del conjunto de datos:

```
  Attribute Information:
   1. sepal length in cm
   2. sepal width in cm
   3. petal length in cm
   4. petal width in cm
   5. class:
      -- Iris Setosa
      -- Iris Versicolour
      -- Iris Virginica
```

3. Haz una gráfica con los datos como se hizo en el ejercicio de introducción utilizando el método **plot()**.

4. Imprime cual es el ancho y largo promedio del sépalo y pétalo.

## Lectura adicional

[Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython](http://shop.oreilly.com/product/0636920023784.do ) por William McKinney.

## Bibliografía

Documentación de la biblioteca [Pandas](http://pandas.pydata.org/)
